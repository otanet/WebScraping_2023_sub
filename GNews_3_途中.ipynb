{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbXgjkU96jkviuDAQYZ6M0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/WebScraping_2023_sub/blob/main/GNews_3_%E9%80%94%E4%B8%AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Python]Googleニュースをスクレイピング(データ抽出、整形)してみた | まさまゆブログ\n",
        "### https://masamayu.com/google%e3%83%8b%e3%83%a5%e3%83%bc%e3%82%b9%e3%82%92%e3%82%b9%e3%82%af%e3%83%ac%e3%82%a4%e3%83%94%e3%83%b3%e3%82%b0%e3%83%87%e3%83%bc%e3%82%bf%e6%8a%bd%e5%87%ba%e3%80%81%e6%95%b4%e5%bd%a2%e3%81%97/"
      ],
      "metadata": {
        "id": "2bEsEbv7ZDY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pythonのお勉強 No.3 HTMLデータの抽出、整形\n",
        "import requests\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "\n",
        "keyword = \"香川真司 OR 久保建英\"\n",
        "url = 'https://news.google.com/search'\n",
        "\n",
        "params = {'hl':'ja', 'gl':'JP', 'ceid':'JP:ja', 'q':keyword}\n",
        "article_no = 1\n",
        "\n",
        "# url、パラメータを設定してリクエストを送る\n",
        "res = requests.get(url, params=params)\n",
        "# レスポンスをBeautifulSoupで解析する\n",
        "soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "\n",
        "# レスポンスからh3階層のニュースを抽出する（classにxrnccdを含むタグ）\n",
        "h3_blocks = soup.select(\".xrnccd\")\n",
        "\n",
        "for i, h3_entry in enumerate(h3_blocks):\n",
        "\n",
        "    # 記事を10件だけ処理する\n",
        "    if article_no == 11:\n",
        "        break\n",
        "    \n",
        "    with open('result.txt', mode='a') as f:\n",
        "        # ニュースのタイトルを抽出する（h3タグ配下のaタグの内容）\n",
        "        h3_title = h3_entry.select_one(\"h3 a\").text\n",
        "        # ニュースのリンクを抽出する（h3タグ配下のaタグのhref属性）\n",
        "        h3_link = h3_entry.select_one(\"h3 a\")[\"href\"]\n",
        "        # 抽出したURLを整形して絶対パスを作る\n",
        "        h3_link = urllib.parse.urljoin(url, h3_link)\n",
        "\n",
        "        # ニュースのタイトル、リンクをファイルに書き込む\n",
        "        f.write(h3_title)\n",
        "        f.write('\\r\\n')\n",
        "        f.write(h3_link)\n",
        "        f.write('\\r\\n')\n",
        "\n",
        "        article_no = article_no + 1\n",
        "\n",
        "        # h3階層のニュースからh4階層のニュースを抽出する\n",
        "        h4_block = h3_entry.select_one(\".SbNwzf\")\n",
        "\n",
        "        if h4_block != None:\n",
        "            # h4階層が存在するときのみニュースを抽出する\n",
        "            h4_articles = h4_block.select(\"article\")\n",
        "\n",
        "            for j, h4_entry in enumerate(h4_articles):\n",
        "                h4_title = h4_entry.select_one(\"h4 a\").text\n",
        "                h4_link = h4_entry.select_one(\"h4 a\")[\"href\"]\n",
        "                h4_link = urllib.parse.urljoin(url, h4_link)\n",
        "\n",
        "                f.write(h4_title)\n",
        "                f.write('\\r\\n')\n",
        "                f.write(h4_link)\n",
        "                f.write('\\r\\n')\n",
        "\n",
        "                article_no = article_no + 1\n",
        "\n",
        "# ファイルをダウンロードする\n",
        "files.download('result.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pWGfCP4XZDDG",
        "outputId": "6d85e3e2-58bb-44fd-bca1-3a469555e521"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d7f99c7-89f6-4547-a651-f76896f260a1\", \"result.txt\", 3314)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pythonのお勉強 No.3 HTMLデータの抽出、整形\n",
        "import requests\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "\n",
        "keyword = \"スパイダープラス\"\n",
        "url = 'https://news.google.com/search'\n",
        "\n",
        "params = {'hl':'ja', 'gl':'JP', 'ceid':'JP:ja', 'q':keyword}\n",
        "article_no = 1\n",
        "\n",
        "# url、パラメータを設定してリクエストを送る\n",
        "res = requests.get(url, params=params)\n",
        "# レスポンスをBeautifulSoupで解析する\n",
        "soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "\n",
        "# レスポンスからh3階層のニュースを抽出する（classにxrnccdを含むタグ）\n",
        "h3_blocks = soup.select(\".xrnccd\")\n",
        "\n",
        "for i, h3_entry in enumerate(h3_blocks):\n",
        "\n",
        "    # 記事を10件だけ処理する\n",
        "    if article_no == 11:\n",
        "        break\n",
        "    \n",
        "    with open('result.txt', mode='a') as f:\n",
        "        # ニュースのタイトルを抽出する（h3タグ配下のaタグの内容）\n",
        "        h3_title = h3_entry.select_one(\"h3 a\").text\n",
        "        # ニュースのリンクを抽出する（h3タグ配下のaタグのhref属性）\n",
        "        h3_link = h3_entry.select_one(\"h3 a\")[\"href\"]\n",
        "        # 抽出したURLを整形して絶対パスを作る\n",
        "        h3_link = urllib.parse.urljoin(url, h3_link)\n",
        "\n",
        "        # ニュースのタイトル、リンクをファイルに書き込む\n",
        "        f.write(h3_title)\n",
        "        f.write('\\r\\n')\n",
        "        f.write(h3_link)\n",
        "        f.write('\\r\\n')\n",
        "\n",
        "        article_no = article_no + 1\n",
        "\n",
        "        # h3階層のニュースからh4階層のニュースを抽出する\n",
        "        h4_block = h3_entry.select_one(\".SbNwzf\")\n",
        "\n",
        "        if h4_block != None:\n",
        "            # h4階層が存在するときのみニュースを抽出する\n",
        "            h4_articles = h4_block.select(\"article\")\n",
        "\n",
        "            for j, h4_entry in enumerate(h4_articles):\n",
        "                h4_title = h4_entry.select_one(\"h4 a\").text\n",
        "                h4_link = h4_entry.select_one(\"h4 a\")[\"href\"]\n",
        "                h4_link = urllib.parse.urljoin(url, h4_link)\n",
        "\n",
        "                f.write(h4_title)\n",
        "                f.write('\\r\\n')\n",
        "                f.write(h4_link)\n",
        "                f.write('\\r\\n')\n",
        "\n",
        "                article_no = article_no + 1\n",
        "\n",
        "# ファイルをダウンロードする\n",
        "files.download('result.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "eSCgD0usZDFa",
        "outputId": "f2982d51-be24-40a7-d378-3350e065d93e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bd58eb0f-5790-46c9-b231-9778460a5b37\", \"result.txt\", 2403)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9bywwFo0ZE8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OnQGyC3TZE-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46X7Q1N6ZFCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWxtnw25ZFDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}