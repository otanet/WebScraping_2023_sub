{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn2S6DFA7gME+DSZAUpl0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/WebScraping_2023_sub/blob/main/GNews_1_%E9%80%94%E4%B8%AD_20230109.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzCWEKqVtZJH",
        "outputId": "6b7c9c44-7678-4260-f4ca-d9997789f955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "res = requests.get('https://www.google.com/search?q=%E3%82%B9%E3%83%91%E3%82%A4%E3%83%80%E3%83%BC%E3%83%97%E3%83%A9%E3%82%B9&sxsrf=AJOqlzVdMcD-skixlwJ2n26gvo3dMnUGVg:1673090517752&source=lnms&tbm=nws&sa=X&ved=2ahUKEwigifPvq7X8AhXXMd4KHVfnAO4Q_AUoAXoECAEQAw&biw=1151&bih=510&dpr=1.65')\n",
        "soup = BeautifulSoup(res.text)\n",
        "\n",
        "article_list = soup.select('div > a jsname')\n",
        "\n",
        "#for a in article_list:\n",
        "#    print(a['href'], a.get_text())\n",
        "print(article_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20230109 1933-\n",
        "### ニュース記事をスクレイピング - Qiita\n",
        "### https://qiita.com/babmuclr/items/64a8833bfe7a7e32f09e\n"
      ],
      "metadata": {
        "id": "f64PnkrmFcL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. urllib.requestとBeautifulSoupを使う方法 2. Seleniumを使う方法\n"
      ],
      "metadata": {
        "id": "yo_WFeyJGEgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "-xjiI0f-FbWw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.google.com/search?q=%E3%82%B9%E3%83%91%E3%82%A4%E3%83%80%E3%83%BC%E3%83%97%E3%83%A9%E3%82%B9&sxsrf=AJOqlzXkTKJFytjRWtYTOJ9FP2W7Ec30Ew:1673260597332&source=lnms&sa=X&ved=2ahUKEwj23pS8pbr8AhXXZd4KHeccDBwQ_AUoAHoECAEQAg&biw=1151&bih=510&dpr=1.65\"\n",
        "url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "eV0Q6JT_Fbbb",
        "outputId": "6320e090-c584-4d60-f6fb-df0d7a7b7423"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://www.google.com/search?q=%E3%82%B9%E3%83%91%E3%82%A4%E3%83%80%E3%83%BC%E3%83%97%E3%83%A9%E3%82%B9&sxsrf=AJOqlzXkTKJFytjRWtYTOJ9FP2W7Ec30Ew:1673260597332&source=lnms&sa=X&ved=2ahUKEwj23pS8pbr8AhXXZd4KHeccDBwQ_AUoAHoECAEQAg&biw=1151&bih=510&dpr=1.65'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = request.urlopen(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "gGMs3EbDFbf3",
        "outputId": "6f754b98-c6a8-4540-987f-f4b28cf8aaf1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-563e738ee388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response,\"xml\")\n",
        "response.close()\n",
        "articles = soup.find_all(\"item\")"
      ],
      "metadata": {
        "id": "xniuJJz-Fbke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 結論: bs4とrequestで、ダイレクトなURLからだと403エラーがでるので、Seleniumで、検索キーワードを入力するところからやるしかない。"
      ],
      "metadata": {
        "id": "1LMBHCyqHJ-_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTeiStjNFblo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pythonでグーグルニュースの検索結果をスクレイピング (2) Beautiful Soupを使う - Qiita\n",
        "### https://qiita.com/tyamaguchi636/items/9b758b194057ac2df4f9\n",
        "### 20230109 1958-"
      ],
      "metadata": {
        "id": "qPFS0ZZkLg4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd    #スクレイピング結果をデータフレーム形式でcvsファイルに保存するため\n",
        "import pprint    #データフレームの一部を表示するため\n",
        "from bs4 import BeautifulSoup  #取得したWebページの情報の解析と抽出\n",
        "import requests     #Webページの情報の取得\n",
        "import urllib       #キーワードのurlエンコード取得"
      ],
      "metadata": {
        "id": "paWfi3buLhDS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"スパイダープラス\"\n",
        "s_quote = urllib.parse.quote(s)\n",
        "url_b4 = 'https://news.google.com/search?q=' + s_quote + '&hl=ja&gl=JP&ceid=JP%3Aja'"
      ],
      "metadata": {
        "id": "1OGYMeqCLhF6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#検索結果ページの情報を取得\n",
        "res = requests.get(url_b4)\n",
        "soup = BeautifulSoup(res.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "G0Dd_t83LhJY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#すべての記事部の情報を選択\n",
        "articles = soup.select(\".iRPxbe\")"
      ],
      "metadata": {
        "id": "MB7ex8PlLhL4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_6fNuJ7Qkik",
        "outputId": "144fcb72-548f-4c1c-bf2a-9bcaefa43fd4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#各記事の情報をfor　～　enumerate分で繰り返し取得してリストに代入\n",
        "news = list()   #代入のための空のリストを作成\n",
        "\n",
        "for i, entry in enumerate(articles, 1):\n",
        "    title = entry.find(\"h3\").text\n",
        "    summary = entry.find(\"span\").text\n",
        "    summary = title + \"。\" + summary\n",
        "    #url_elm = entry.find(\"a\")を下記に変更\n",
        "    url_elm = entry.find(\"article\")\n",
        "    link = url_elm.get(\"jslog\")\n",
        "    link = link.lstrip(\"85008; 2:\")     #左端削除\n",
        "    link = link.rstrip(\"; track:click\") #右端削除\n",
        "    time_elm = entry.find(\"time\")\n",
        "    try:    #例外処理\n",
        "        ymd = time_elm.get(\"datetime\")\n",
        "    except AttributeError:\n",
        "        ymd = \"0000-00-00\"\n",
        "    ymd = ymd[0:10]\n",
        "    ymd = ymd.replace(\"-\", \"/\")     #置換\n",
        "    sortkey = ymd[0:4] + ymd[5:7] + ymd[8:10] #年月日でのソート用\n",
        "\n",
        "    tmp = {             #辞書型で格納\n",
        "        \"title\": title,\n",
        "        \"summary\": summary,\n",
        "        \"link\": link,\n",
        "        \"published\": ymd,\n",
        "        \"sortkey\": sortkey\n",
        "        }\n",
        "\n",
        "    news.append(tmp)  #各記事の情報をリストに追加\n",
        "\n",
        "    #データフレームに変換してcsvファイルで保存\n",
        "    news_df = pd.DataFrame(news)\n",
        "    pprint.pprint(news_df.head())  #最初の5行を表示してデータ確認 \n",
        "    filename = s + \".csv\"\n",
        "    news_df.to_csv(filename, encoding='utf-8-sig', index=False) "
      ],
      "metadata": {
        "id": "NBgkEVm2LhP1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzsI4fprQIcz",
        "outputId": "b27636de-ec33-45f8-f069-c07b6345c6e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZHGzccIRLhQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリの呼び出し\n",
        "import pandas as pd    #スクレイピング結果をデータフレーム形式でcvsファイルに保存するため\n",
        "import pprint    #データフレームの一部を表示するため\n",
        "from bs4 import BeautifulSoup  #取得したWebページの情報の解析と抽出\n",
        "import requests     #Webページの情報の取得\n",
        "import urllib       #キーワードのurlエンコード取得\n",
        "\n",
        "#検索ワード「タピる」を文字変換して検索結果ページのurlの間に挿入\n",
        "s = \"タピる\"\n",
        "s_quote = urllib.parse.quote(s)\n",
        "url_b4 = 'https://news.google.com/search?q=' + s_quote + '&hl=ja&gl=JP&ceid=JP%3Aja'\n",
        "\n",
        "#検索結果ページの情報を取得\n",
        "res = requests.get(url_b4)\n",
        "soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "\n",
        "#すべての記事部の情報を選択\n",
        "articles = soup.select(\".xrnccd\")\n",
        "\n",
        "#各記事の情報をfor　～　enumerate分で繰り返し取得してリストに代入\n",
        "news = list()   #代入のための空のリストを作成\n",
        "\n",
        "for i, entry in enumerate(articles, 1):\n",
        "    title = entry.find(\"h3\").text\n",
        "    summary = entry.find(\"span\").text\n",
        "    summary = title + \"。\" + summary\n",
        "    #url_elm = entry.find(\"a\")を下記に変更\n",
        "    url_elm = entry.find(\"article\")\n",
        "    link = url_elm.get(\"jslog\")\n",
        "    link = link.lstrip(\"85008; 2:\")     #左端削除\n",
        "    link = link.rstrip(\"; track:click\") #右端削除\n",
        "    time_elm = entry.find(\"time\")\n",
        "    try:    #例外処理\n",
        "        ymd = time_elm.get(\"datetime\")\n",
        "    except AttributeError:\n",
        "        ymd = \"0000-00-00\"\n",
        "    ymd = ymd[0:10]\n",
        "    ymd = ymd.replace(\"-\", \"/\")     #置換\n",
        "    sortkey = ymd[0:4] + ymd[5:7] + ymd[8:10] #年月日でのソート用\n",
        "\n",
        "    tmp = {             #辞書型で格納\n",
        "        \"title\": title,\n",
        "        \"summary\": summary,\n",
        "        \"link\": link,\n",
        "        \"published\": ymd,\n",
        "        \"sortkey\": sortkey\n",
        "        }\n",
        "\n",
        "    news.append(tmp)  #各記事の情報をリストに追加\n",
        "\n",
        "    #データフレームに変換してcsvファイルで保存\n",
        "    news_df = pd.DataFrame(news)\n",
        "    pprint.pprint(news_df.head())  #最初の5行を表示してデータ確認 \n",
        "    filename = s + \".csv\"\n",
        "    news_df.to_csv(filename, encoding='utf-8-sig', index=False) "
      ],
      "metadata": {
        "id": "oNKPYr6SLhRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリの呼び出し\n",
        "import pandas as pd    #スクレイピング結果をデータフレーム形式でcvsファイルに保存するため\n",
        "import pprint    #データフレームの一部を表示するため\n",
        "from bs4 import BeautifulSoup  #取得したWebページの情報の解析と抽出\n",
        "import requests     #Webページの情報の取得\n",
        "import urllib       #キーワードのurlエンコード取得\n",
        "\n",
        "#検索ワード「タピる」を文字変換して検索結果ページのurlの間に挿入\n",
        "s = \"スパイダープラス\"\n",
        "s_quote = urllib.parse.quote(s)\n",
        "url_b4 = 'https://news.google.com/search?q=' + s_quote + '&hl=ja&gl=JP&ceid=JP%3Aja'\n",
        "\n",
        "#検索結果ページの情報を取得\n",
        "res = requests.get(url_b4)\n",
        "soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "\n",
        "#すべての記事部の情報を選択\n",
        "articles = soup.select(\".xrnccd\")\n",
        "\n",
        "#各記事の情報をfor　～　enumerate分で繰り返し取得してリストに代入\n",
        "news = list()   #代入のための空のリストを作成\n",
        "\n",
        "for i, entry in enumerate(articles, 1):\n",
        "    title = entry.find(\"h3\").text\n",
        "    summary = entry.find(\"span\").text\n",
        "    summary = title + \"。\" + summary\n",
        "    #url_elm = entry.find(\"a\")を下記に変更\n",
        "    url_elm = entry.find(\"article\")\n",
        "    link = url_elm.get(\"jslog\")\n",
        "    link = link.lstrip(\"85008; 2:\")     #左端削除\n",
        "    link = link.rstrip(\"; track:click\") #右端削除\n",
        "    time_elm = entry.find(\"time\")\n",
        "    try:    #例外処理\n",
        "        ymd = time_elm.get(\"datetime\")\n",
        "    except AttributeError:\n",
        "        ymd = \"0000-00-00\"\n",
        "    ymd = ymd[0:10]\n",
        "    ymd = ymd.replace(\"-\", \"/\")     #置換\n",
        "    sortkey = ymd[0:4] + ymd[5:7] + ymd[8:10] #年月日でのソート用\n",
        "\n",
        "    tmp = {             #辞書型で格納\n",
        "        \"title\": title,\n",
        "        \"summary\": summary,\n",
        "        \"link\": link,\n",
        "        \"published\": ymd,\n",
        "        \"sortkey\": sortkey\n",
        "        }\n",
        "\n",
        "    news.append(tmp)  #各記事の情報をリストに追加\n",
        "\n",
        "    #データフレームに変換してcsvファイルで保存\n",
        "    news_df = pd.DataFrame(news)\n",
        "    pprint.pprint(news_df.head())  #最初の5行を表示してデータ確認 \n",
        "    filename = s + \".csv\"\n",
        "    news_df.to_csv(filename, encoding='utf-8-sig', index=False) "
      ],
      "metadata": {
        "id": "_d0Bq2T_t44v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wOkQT5D9RH3T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9mwJPB9Uwd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rxk3LD8zUwhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uk7u-DqzUwjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lAZscatUwnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gg-VpGBxUwpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aV8zpuHUUwsX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}